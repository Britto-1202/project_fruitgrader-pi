# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lQgCz_lklt-eYe4vNbq5QvGqp8EYILte
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from keras.utils import to_categorical
from keras.utils  import  load_img
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import EarlyStopping,ModelCheckpoint
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from keras.models import Model,Sequential
from keras.layers import Conv2D, GlobalAveragePooling2D, Dropout, Flatten, Dense, Activation,GlobalMaxPooling2D
from keras.applications.inception_v3 import InceptionV3
from sklearn.metrics import confusion_matrix,classification_report
from keras.optimizers import Adam
os.listdir('/content/drive/MyDrive/dataset')

TRAIN_PATH = "/content/drive/MyDrive/dataset/train"
TEST_PATH = "/content/drive/MyDrive/dataset/test"
BATCH_SIZE = 32
EPOCHS = 10
LEARNING_RATE = 0.001
IMG_SHAPE= (224,224)

train_datagen = ImageDataGenerator(rescale=1/255.0,
                                 zoom_range=0.2,
                                 shear_range=0.3,
                                 horizontal_flip=True,
                                 brightness_range=[0.5,1.5])

test_datagen = ImageDataGenerator(rescale=1/255.0)


train_gen = train_datagen.flow_from_directory(TRAIN_PATH,
                                            target_size=IMG_SHAPE,
                                            batch_size=BATCH_SIZE,
                                            class_mode="binary")

test_gen = test_datagen.flow_from_directory(TEST_PATH,
                                            target_size=IMG_SHAPE,
                                            batch_size=BATCH_SIZE,
                                            class_mode="binary")

#get classes dict
classes_dict = dict(test_gen.class_indices)
#reverse
classes_dict = {v: k for k,v in classes_dict.items()}
#let's plot sone images
images,labels=train_gen.next()
plt.figure(figsize=(20,10))
for i in range(12):
    plt.subplot(3,4,i+1)
    plt.imshow(images[i])
    plt.xticks([])
    plt.yticks([])
    plt.xlabel(classes_dict[labels[i]])

inception = InceptionV3(weights='imagenet',input_shape=(224, 224, 3),include_top=False)

inception.summary()

layers = inception.layers
print(f'Number of Layers: {len(layers)}')

TRAIN_SIZE = train_gen.samples
TEST_SIZE = test_gen.samples

callbacks = EarlyStopping(patience = 3, monitor='val_acc')


inputs = inception.input

x = inception.output
x = GlobalAveragePooling2D()(x)

x = Dense(512, activation='relu')(x)

x = Dropout(0.5)(x)

outputs = Dense(6, activation ='softmax')(x)

model = Model(inputs=inputs, outputs=outputs)


for layer in layers:
    layer.trainable = False

model.compile(optimizer=Adam(LEARNING_RATE),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(
                train_gen,
                epochs= EPOCHS,
                validation_data = test_gen,
                validation_steps = TEST_SIZE//BATCH_SIZE,
                steps_per_epoch = TRAIN_SIZE//BATCH_SIZE,
                callbacks = [callbacks])

import os

import tensorflow as tf
from tensorflow import keras
model.save('fruits_datas.h5')

model=keras.models.load_model('/content/drive/MyDrive/fruits_datas.h5')

import tensorflow as tf

# Load the model
model = tf.keras.models.load_model('/content/drive/MyDrive/fruits_datas.h5')

# Convert the model
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the converted model
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)

model.evaluate_tflite('model.tflite', val_data)

from tensorflow import keras
from keras.preprocessing import image
import numpy as np
from keras.utils import load_img,img_to_array
img_height, img_width = 224, 224
img_path = '/content/drive/MyDrive/fruts/dataset/dataset/test/rottenapples/Screen Shot 2018-06-07 at 2.15.34 PM.png'
img =load_img(img_path, target_size=(img_height, img_width))
x = img_to_array(img)
x = np.expand_dims(x, axis=0)
x = x / 255.0
preds = model.predict(x)
print(preds)
if preds[0][0] > 0.5:
    print("The fruit is a fresh  apple .")
elif preds[0][1] > 0.5:
  print("The fruit is a fresh banana .")
elif preds[0][2] > 0.5:
  print("The fruit is a fresh  orange .")
elif preds[0][3] > 0.5:
  print("The fruit is  a rotten apple.")
elif preds[0][4] > 0.5:
  print("The fruit is a rotten banana.")
else:
    print("The fruit is a rotten orange.")

import cv2
import numpy as np
import tensorflow as tf

# Load the pre-trained ML model
model = tf.keras.models.load_model('/content/drive/MyDrive/fruits_datas.h5')

# Start capturing video from webcam
cap = cv2.VideoCapture(1)

# Set the frame size
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)


while True:
    # Read a frame from the video stream
    ret, frame = cap.read()

    # Display the original frame
    cv2.imshow('Fruit Detector', frame)

    # Wait for key press
    key = cv2.waitKey(1)
    if key == ord('q'):
        break
    elif key == ord(' '):
        # Capture the image and save it to a file
        fruit = frame
        cv2.imwrite('fruit.jpg', fruit)

        # Preprocess the image for the ML model
        img = tf.keras.preprocessing.image.img_to_array(fruit)
        img = tf.keras.preprocessing.image.smart_resize(img, (224, 224))
        img = tf.keras.applications.mobilenet_v2.preprocess_input(img)

        # Pass the image to the ML model and get the prediction
        preds = model.predict(np.expand_dims(img, axis=0))
        print(preds)

        # Define a dictionary to map indices to fruit labels
        label_dict = {0: "fresh apple", 1: "fresh banana", 2: "fresh orange",
                      3: "rotten apple", 4: "rotten banana", 5: "rotten orange"}

        # Get the index of the maximum value in the preds array
        pred_index = np.argmax(preds)
        print(pred_index)
        # Check if the predicted class is above the threshold
        if preds[0][pred_index] > 0.5:
            # Get the corresponding fruit label from the   dictionary
            label = label_dict[pred_index]
            print(f"The fruit is a {label}.")
        else:
            print("Please show a valid image.")

# Release the capture and destroy all windows
cap.release()
cv2.destroyAllWindows()

plt.style.use('ggplot')
plt.figure()
fig,(ax1, ax2)=plt.subplots(1,2,figsize=(19,8))
ax1.plot(history.history['loss'])
ax1.plot(history.history['val_loss'])
ax1.legend(['Training','Validation'])
ax1.set_title('Loss')
ax1.set_xlabel('N. of Epochs')
ax2.plot(history.history['accuracy'])
ax2.plot(history.history['val_accuracy'])
ax2.legend(['Training','Validation'])
ax2.set_title('Acurracy')
ax2.set_xlabel('N.of Epochs')

loss, test_acc = model.evaluate(test_gen)
print("Validation Accuracy = %f \nValidation Loss = %f " % (test_acc, loss))

class_names = list(classes_dict.values())
labels = test_gen.classes
preds =  model.predict(test_gen)
predictions = np.argmax(preds, axis=1)
conf_matrix = confusion_matrix(labels, predictions)
fig,ax = plt.subplots(figsize=(12, 10))
sb.heatmap(conf_matrix, annot=True, linewidths=0.01,cmap="magma",linecolor="gray", fmt= '.1f',ax=ax)
plt.xlabel("Predicted Class")
plt.ylabel("True Class")
plt.title("Confusion Matrix")
ax.set_xticklabels(labels = class_names,fontdict=None)
ax.set_yticklabels(labels = class_names,fontdict=None)
plt.show()

test_images,test_labels=test_gen.next()
plt.figure(figsize=(20,15))
for i in range(10):
    plt.subplot(2,5,i+1)
    plt.imshow(test_images[i])
    plt.xticks([])
    plt.yticks([])
    real = classes_dict[test_labels[i]]
    img = test_images[i].reshape(1,224,224,3)
    predicted = int(np.argmax(model.predict(img),axis=1))
    predicted = classes_dict[predicted]
    plt.xlabel(f"Real: {real}\n Predicted: {predicted}")